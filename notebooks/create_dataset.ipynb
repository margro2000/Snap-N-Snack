{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/davinan/Dropbox/github/Snap-N-Snack/data/snapnsnackdb'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "os.chdir(\"../data/snapnsnackdb\")\n",
    "import sys\n",
    "sys.path.append(\"/Users/davinan/Dropbox/github/GoogleImagesDownloader/\")\n",
    "from download_with_selenium import download_images as dim_selenium\n",
    "from download_with_urllib import  download_images as dim_urllib\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle as pkl\n",
    "epi = pd.read_csv(\"epi_r.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>rating</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>#cakeweek</th>\n",
       "      <th>#wasteless</th>\n",
       "      <th>22-minute meals</th>\n",
       "      <th>3-ingredient recipes</th>\n",
       "      <th>...</th>\n",
       "      <th>yellow squash</th>\n",
       "      <th>yogurt</th>\n",
       "      <th>yonkers</th>\n",
       "      <th>yuca</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>cookbooks</th>\n",
       "      <th>leftovers</th>\n",
       "      <th>snack</th>\n",
       "      <th>snack week</th>\n",
       "      <th>turkey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lentil, Apple, and Turkey Wrap</td>\n",
       "      <td>2.500</td>\n",
       "      <td>426.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>559.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boudin Blanc Terrine with Red Onion Confit</td>\n",
       "      <td>4.375</td>\n",
       "      <td>403.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1439.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Potato and Fennel Soup Hodge</td>\n",
       "      <td>3.750</td>\n",
       "      <td>165.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mahi-Mahi in Tomato Olive Sauce</td>\n",
       "      <td>5.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spinach Noodle Casserole</td>\n",
       "      <td>3.125</td>\n",
       "      <td>547.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20047</th>\n",
       "      <td>Parmesan Puffs</td>\n",
       "      <td>3.125</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20048</th>\n",
       "      <td>Artichoke and Parmesan Risotto</td>\n",
       "      <td>4.375</td>\n",
       "      <td>671.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>583.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20049</th>\n",
       "      <td>Turkey Cream Puff Pie</td>\n",
       "      <td>4.375</td>\n",
       "      <td>563.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>Snapper on Angel Hair with Citrus Cream</td>\n",
       "      <td>4.375</td>\n",
       "      <td>631.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>517.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20051</th>\n",
       "      <td>Baked Ham with Marmalade-Horseradish Glaze</td>\n",
       "      <td>4.375</td>\n",
       "      <td>560.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3698.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20052 rows × 680 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             title  rating  calories  protein  \\\n",
       "0                  Lentil, Apple, and Turkey Wrap    2.500     426.0     30.0   \n",
       "1      Boudin Blanc Terrine with Red Onion Confit    4.375     403.0     18.0   \n",
       "2                    Potato and Fennel Soup Hodge    3.750     165.0      6.0   \n",
       "3                 Mahi-Mahi in Tomato Olive Sauce    5.000       NaN      NaN   \n",
       "4                        Spinach Noodle Casserole    3.125     547.0     20.0   \n",
       "...                                            ...     ...       ...      ...   \n",
       "20047                              Parmesan Puffs    3.125      28.0      2.0   \n",
       "20048              Artichoke and Parmesan Risotto    4.375     671.0     22.0   \n",
       "20049                       Turkey Cream Puff Pie    4.375     563.0     31.0   \n",
       "20050     Snapper on Angel Hair with Citrus Cream    4.375     631.0     45.0   \n",
       "20051  Baked Ham with Marmalade-Horseradish Glaze    4.375     560.0     73.0   \n",
       "\n",
       "        fat  sodium  #cakeweek  #wasteless  22-minute meals  \\\n",
       "0       7.0   559.0        0.0         0.0              0.0   \n",
       "1      23.0  1439.0        0.0         0.0              0.0   \n",
       "2       7.0   165.0        0.0         0.0              0.0   \n",
       "3       NaN     NaN        0.0         0.0              0.0   \n",
       "4      32.0   452.0        0.0         0.0              0.0   \n",
       "...     ...     ...        ...         ...              ...   \n",
       "20047   2.0    64.0        0.0         0.0              0.0   \n",
       "20048  28.0   583.0        0.0         0.0              0.0   \n",
       "20049  38.0   652.0        0.0         0.0              0.0   \n",
       "20050  24.0   517.0        0.0         0.0              0.0   \n",
       "20051  10.0  3698.0        0.0         0.0              0.0   \n",
       "\n",
       "       3-ingredient recipes  ...  yellow squash  yogurt  yonkers  yuca  \\\n",
       "0                       0.0  ...            0.0     0.0      0.0   0.0   \n",
       "1                       0.0  ...            0.0     0.0      0.0   0.0   \n",
       "2                       0.0  ...            0.0     0.0      0.0   0.0   \n",
       "3                       0.0  ...            0.0     0.0      0.0   0.0   \n",
       "4                       0.0  ...            0.0     0.0      0.0   0.0   \n",
       "...                     ...  ...            ...     ...      ...   ...   \n",
       "20047                   0.0  ...            0.0     0.0      0.0   0.0   \n",
       "20048                   0.0  ...            0.0     0.0      0.0   0.0   \n",
       "20049                   0.0  ...            0.0     0.0      0.0   0.0   \n",
       "20050                   0.0  ...            0.0     0.0      0.0   0.0   \n",
       "20051                   0.0  ...            0.0     0.0      0.0   0.0   \n",
       "\n",
       "       zucchini  cookbooks  leftovers  snack  snack week  turkey  \n",
       "0           0.0        0.0        0.0    0.0         0.0     1.0  \n",
       "1           0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "2           0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "3           0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "4           0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "...         ...        ...        ...    ...         ...     ...  \n",
       "20047       0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "20048       0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "20049       0.0        0.0        0.0    0.0         0.0     1.0  \n",
       "20050       0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "20051       0.0        0.0        0.0    0.0         0.0     0.0  \n",
       "\n",
       "[20052 rows x 680 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\n",
      "[                                                                        ]   0%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[========================================================================] 100%\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from google_images_download import google_images_download as gid\n",
    "from simple_image_download import simple_image_download as simp\n",
    "\n",
    "\n",
    "# creating object \n",
    "# response = gid.googleimagesdownload()  \n",
    "  \n",
    "search_queries = epi[\"title\"].values[:2]\n",
    "  \n",
    "def downloadimages(query, limit = 10): \n",
    "    response = simp.simple_image_download\n",
    "    try:\n",
    "        response().download(query, limit =limit) \n",
    "      \n",
    "    # Handling File NotFound Error     \n",
    "    except FileNotFoundError:  \n",
    "        # Providing arguments for the searched query \n",
    "#         try: \n",
    "#             # Downloading the photos based \n",
    "#             # on the given arguments \n",
    "#             response.download(query, extensions={'.jpg'}, limit=limit)  \n",
    "#         except: \n",
    "#             pass\n",
    "        print(f\"missed {query}\")\n",
    "        \n",
    "for query in search_queries: \n",
    "#     downloadimages(query.replace(\" \", \"\").replace(\"and\", \"\"))  \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_queries = epi[\"title\"].values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lentil,Apple,TurkeyWrap'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries[0].replace(\" \", \"\").replace(\"and\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lentil, Apple, and Turkey Wrap'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import urllib\n",
    "import requests\n",
    "import magic\n",
    "import progressbar\n",
    "from urllib.parse import quote\n",
    "\n",
    "class simple_image_download_key:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def urls(self, keywords, limit, extensions={'.jpg', '.png', '.ico', '.gif', '.jpeg'}):\n",
    "        keyword_to_search = [str(item).strip() for item in keywords.split(',')]\n",
    "#         keyword_to_search = [keywords.strip().replace(\" \", \"+\")]\n",
    "        i = 0\n",
    "        links = []\n",
    "\n",
    "        things = len(keyword_to_search) * limit\n",
    "\n",
    "        bar = progressbar.ProgressBar(maxval=things, \\\n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()\n",
    "\n",
    "        while i < len(keyword_to_search):\n",
    "            url = 'https://www.google.com/search?q=' + quote(\n",
    "                keyword_to_search[i].encode(\n",
    "                    'utf-8')) + '&biw=1536&bih=674&tbm=isch&sxsrf=ACYBGNSXXpS6YmAKUiLKKBs6xWb4uUY5gA:1581168823770&source=lnms&sa=X&ved=0ahUKEwioj8jwiMLnAhW9AhAIHbXTBMMQ_AUI3QUoAQ'\n",
    "            raw_html = self._download_page(url)\n",
    "\n",
    "            end_object = -1;\n",
    "            google_image_seen = False;\n",
    "            j = 0\n",
    "\n",
    "            while j < limit:\n",
    "                while (True):\n",
    "                    try:\n",
    "                        new_line = raw_html.find('\"https://', end_object + 1)\n",
    "                        end_object = raw_html.find('\"', new_line + 1)\n",
    "\n",
    "                        buffor = raw_html.find('\\\\', new_line + 1, end_object)\n",
    "                        if buffor != -1:\n",
    "                            object_raw = (raw_html[new_line + 1:buffor])\n",
    "                        else:\n",
    "                            object_raw = (raw_html[new_line + 1:end_object])\n",
    "\n",
    "                        if any(extension in object_raw for extension in extensions):\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        break\n",
    "\n",
    "\n",
    "                try:\n",
    "                    r = requests.get(object_raw, allow_redirects=True, timeout=1)\n",
    "                    if('html' not in str(r.content)):\n",
    "                        mime = magic.Magic(mime=True)\n",
    "                        file_type = mime.from_buffer(r.content)\n",
    "                        file_extension = f'.{file_type.split(\"/\")[1]}'\n",
    "                        if file_extension == '.png' and not google_image_seen:\n",
    "                            google_image_seen = True\n",
    "                            raise ValueError();\n",
    "                        links.append(object_raw)\n",
    "                        bar.update(bar.currval + 1)\n",
    "                    else:\n",
    "                        j -= 1\n",
    "                except Exception as e:\n",
    "                    j -= 1\n",
    "                j += 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        bar.finish()\n",
    "        return(links)\n",
    "\n",
    "\n",
    "    def download(self, keywords, limit, extensions={'.jpg', '.png', '.ico', '.gif', '.jpeg'}):\n",
    "#         keyword_to_search = [str(item).strip() for item in keywords.split(',')]\n",
    "        keyword_to_search = [keywords.replace(\" \", \"+\").replace(\",\", \"\")]\n",
    "        main_directory = \"simple_images/\"\n",
    "        i = 0\n",
    "\n",
    "        things = len(keyword_to_search) * limit\n",
    "\n",
    "        bar = progressbar.ProgressBar(maxval=things, \\\n",
    "                                      widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()])\n",
    "\n",
    "        bar.start()\n",
    "\n",
    "        while i < len(keyword_to_search):\n",
    "            self._create_directories(main_directory, keyword_to_search[i])\n",
    "            url = 'https://www.google.com/search?q=' + quote(\n",
    "                keyword_to_search[i].encode('utf-8')) + '&biw=1536&bih=674&tbm=isch&sxsrf=ACYBGNSXXpS6YmAKUiLKKBs6xWb4uUY5gA:1581168823770&source=lnms&sa=X&ved=0ahUKEwioj8jwiMLnAhW9AhAIHbXTBMMQ_AUI3QUoAQ'\n",
    "            raw_html = self._download_page(url)\n",
    "\n",
    "\n",
    "            end_object = -1;\n",
    "            google_image_seen = False;\n",
    "            j = 0\n",
    "            while j < limit:\n",
    "                while (True):\n",
    "                    try:\n",
    "                        new_line = raw_html.find('\"https://', end_object + 1)\n",
    "                        end_object = raw_html.find('\"', new_line + 1)\n",
    "\n",
    "                        buffor = raw_html.find('\\\\', new_line + 1, end_object)\n",
    "                        if buffor != -1:\n",
    "                            object_raw = (raw_html[new_line+1:buffor])\n",
    "                        else:\n",
    "                            object_raw = (raw_html[new_line+1:end_object])\n",
    "\n",
    "                        if any(extension in object_raw for extension in extensions):\n",
    "                            break\n",
    "\n",
    "                    except Exception as e:\n",
    "                        break\n",
    "                path = main_directory + keyword_to_search[i].replace(\" \", \"_\")\n",
    "\n",
    "                try:\n",
    "                    r = requests.get(object_raw, allow_redirects=True, timeout=1)\n",
    "                    if('html' not in str(r.content)):\n",
    "                        mime = magic.Magic(mime=True)\n",
    "                        file_type = mime.from_buffer(r.content)\n",
    "                        file_extension = f'.{file_type.split(\"/\")[1]}'\n",
    "                        if file_extension not in extensions:\n",
    "                            raise ValueError()\n",
    "                        if file_extension == '.png' and not google_image_seen:\n",
    "                            google_image_seen = True\n",
    "                            raise ValueError()\n",
    "                        file_name = str(keyword_to_search[i]) + \"_\" + str(j + 1) + file_extension\n",
    "                        with open(os.path.join(path, file_name), 'wb') as file:\n",
    "                            file.write(r.content)\n",
    "                        bar.update(bar.currval + 1)\n",
    "                    else:\n",
    "                        j -= 1\n",
    "                except Exception as e:\n",
    "                    j -= 1\n",
    "                j += 1\n",
    "\n",
    "            i += 1\n",
    "        bar.finish()\n",
    "\n",
    "\n",
    "    def _create_directories(self, main_directory, name):\n",
    "        name = name.replace(\" \", \"_\")\n",
    "        try:\n",
    "            if not os.path.exists(main_directory):\n",
    "                os.makedirs(main_directory)\n",
    "                time.sleep(0.2)\n",
    "                path = (name)\n",
    "                sub_directory = os.path.join(main_directory, path)\n",
    "                if not os.path.exists(sub_directory):\n",
    "                    os.makedirs(sub_directory)\n",
    "            else:\n",
    "                path = (name)\n",
    "                sub_directory = os.path.join(main_directory, path)\n",
    "                if not os.path.exists(sub_directory):\n",
    "                    os.makedirs(sub_directory)\n",
    "\n",
    "        except OSError as e:\n",
    "            if e.errno != 17:\n",
    "                raise\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    def _download_page(self,url):\n",
    "\n",
    "        try:\n",
    "            headers = {}\n",
    "            headers['User-Agent'] = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36\"\n",
    "            req = urllib.request.Request(url, headers=headers)\n",
    "            resp = urllib.request.urlopen(req)\n",
    "            respData = str(resp.read())\n",
    "            return respData\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_image_download_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ad1ca35e1ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearch_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1159\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdownloadimages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-ad1ca35e1ffc>\u001b[0m in \u001b[0;36mdownloadimages\u001b[0;34m(query, limit)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdownloadimages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_image_download_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'simple_image_download_key' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "search_queries = epi[\"title\"].values\n",
    "  \n",
    "def downloadimages(query, limit = 10): \n",
    "    response = simple_image_download_key\n",
    "    try:\n",
    "        response().download(query, limit =limit) \n",
    "      \n",
    "    # Handling File NotFound Error     \n",
    "    except FileNotFoundError:  \n",
    "        print(f\"missed {query}\")\n",
    "        \n",
    "for query in search_queries[1159:10000]: \n",
    "    downloadimages(query)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lentil+Apple+and+Turkey+Wrap'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_queries[0].strip().replace(\" \", \"+\").replace(\",\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = {}\n",
    "\n",
    "for i, r in epi.iterrows():\n",
    "    label = filt(r[\"title\"]) # convert label to better string\n",
    "#     label = str(label.encode(\"ascii\", \"ignore\"))\n",
    "    target = torch.tensor([r[\"rating\"], r[\"calories\"], r[\"protein\"], r[\"fat\"], r[\"sodium\"]])\n",
    "    targets[label] = target\n",
    "\n",
    "\n",
    "pkl.dump(targets, open(\"targets_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = pkl.load(open(\"targets_dict.pkl\", \"rb\"))\n",
    "imgs = os.listdir(\"simple_images\")\n",
    "downloaded = []\n",
    "for i in imgs:\n",
    "    if filt(i) in targets.keys():\n",
    "        downloaded.append(i)\n",
    "    else:\n",
    "        print(filt(str(i[:-1].encode(\"ascii\", \"ignore\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1159"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Fried\" Chicken \n",
      "\"Fried\"+Chicken\n"
     ]
    }
   ],
   "source": [
    "for i, r in epi.iterrows():\n",
    "    if \"Fried\" in r[\"title\"] and \"Chicken\" in r[\"title\"]:\n",
    "        print(r[\"title\"])\n",
    "        label = r[\"title\"].strip().replace(\" \", \"+\").replace(\",\", \"\")\n",
    "        print(label)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label in targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
      "\u001b[K     |████████████████████████████████| 238 kB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "from unidecode import unidecode\n",
    "def filt(string):\n",
    "    string = unidecode(string)\n",
    "    string = string.strip().replace(\" \", \"+\").replace(\",\", \"\")\n",
    "    string = ''.join(e for e in string if e.isalnum())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gougeres'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt(\"Gougères\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_word(text):\n",
    "    regex = r\"(\\w|\\s)*\"\n",
    "    matches = re.finditer(regex, text, re.DOTALL)\n",
    "    newstr = ''\n",
    "    for matchNum, match in enumerate(matches):\n",
    "        matchNum = matchNum + 1\n",
    "        newstr = newstr + match.group()\n",
    "    return newstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gougeres'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_word(\"Gougères\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(epi.iloc[3][\"protein\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi = torch.tensor([epi.iloc[3][\"protein\"]])\n",
    "oi.repeat(10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(torch.isnan(oi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_path = \"../../data/snapnsnackdb/simple_images\"\n",
    "for path in os.listdir(imgs_path):\n",
    "    for f in os.listdir(os.path.join(imgs_path, path)):\n",
    "        bf = os.path.join(imgs_path, path, f)\n",
    "        af= os.path.join(imgs_path, path, filt(f.split(\".\")[0]) + \".\" + f.split(\".\")[1])\n",
    "#         print(af)\n",
    "#         break\n",
    "#     break\n",
    "        os.rename(bf, af)\n",
    "#         if \".jpg\" in f or \".jpeg\" in f:\n",
    "#             category = os.path.basename(path)\n",
    "#             category = self.filt(category)\n",
    "#             if not torch.any(torch.isnan(self.targets_dict[category])):\n",
    "#                 if len(plt.imread(os.path.join(imgs_path, path, f)).shape) == 3:\n",
    "#                     imgs_paths.append((category, os.path.join(imgs_path, path, f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "def filt(string):\n",
    "    string = unidecode(string)\n",
    "    string = string.strip().replace(\" \", \"+\").replace(\",\", \"\")\n",
    "    string = ''.join(e for e in string if e.isalnum())\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
