
  | Name     | Type   | Params
------------------------------------
0 | backbone | ResNet | 11 M  
/opt/anaconda3/envs/py36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  warnings.warn(*args, **kwargs)
tensor([[0.2109, 0.2220, 0.2104, 0.1889, 0.1678]], grad_fn=<SoftmaxBackward>)
tensor([[   3.7500,  865.0000,   43.0000,   52.0000, 1568.0000]])
loss: 	642093.375
tensor([[0.1647, 0.2527, 0.1880, 0.1560, 0.2385]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 374.0000,   7.0000,  20.0000, 199.0000]])
loss: 	35930.21875
tensor([[0.1304, 0.3116, 0.1615, 0.1251, 0.2714]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 648.0000,   5.0000,  18.0000, 109.0000]])
loss: 	86336.609375
tensor([[0.0956, 0.4080, 0.1407, 0.0989, 0.2568]], grad_fn=<SoftmaxBackward>)
tensor([[  0., 173.,   1.,   0.,   3.]])
loss: 	5959.255859375
tensor([[0.0672, 0.5226, 0.1150, 0.0755, 0.2197]], grad_fn=<SoftmaxBackward>)
tensor([[3.7500,    nan,    nan,    nan,    nan]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 239.0000,   6.0000,   7.0000, 129.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 200.0000,   8.0000,   3.0000, 486.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[   3.7500,  145.0000,    9.0000,    3.0000, 1528.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[4.3750,    nan,    nan,    nan,    nan]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 659.0000,  41.0000,  53.0000, 461.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[0., nan, nan, nan, nan]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.1250, 303.0000,   8.0000,  26.0000, 202.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 401.0000,  13.0000,   5.0000, 420.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 318.0000,  38.0000,  10.0000, 222.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[2.5000,    nan,    nan,    nan,    nan]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[ 0., 53.,  1.,  0.,  1.]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[3.1250,    nan,    nan,    nan,    nan]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 538.0000,  55.0000,  31.0000, 156.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 495.0000,  14.0000,  21.0000, 864.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  5., 203.,   3.,   9.,  34.]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 483.0000,   5.0000,  35.0000, 100.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  5., 395.,   1.,  28.,  88.]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[4.3750e+00, 7.1410e+03, 4.7000e+02, 5.4000e+02, 6.1520e+03]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  5., 485.,   7.,  23., 339.]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[   4.3750,  431.0000,    8.0000,   21.0000, 1218.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 295.0000,  25.0000,  20.0000, 370.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.7500, 444.0000,  27.0000,  30.0000, 405.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 310.0000,   4.0000,  13.0000, 132.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  4.3750, 270.0000,   2.0000,  22.0000, 113.0000]])
loss: 	nan
tensor([[nan, nan, nan, nan, nan]], grad_fn=<SoftmaxBackward>)
tensor([[  3.1250, 341.0000,   3.0000,  21.0000, 201.0000]])
loss: 	nan
/opt/anaconda3/envs/py36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...
  warnings.warn(*args, **kwargs)

GPU available: False, used: False
TPU available: False, using: 0 TPU cores
/opt/anaconda3/envs/py36/lib/python3.6/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop
  warnings.warn(*args, **kwargs)
